{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzMsQL-VaLgz"
   },
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "Credit: Jake Austin, Aryan Jain, Rohan Viswanathan, Stanford CS 231N\n",
    "\n",
    "In this notebook, we will learn about PyTorch, a deep learning framework that will allow us to quickly implement and train models.\n",
    "\n",
    "At the end of the day, **deep learning is just math** that we could implement with some barebones library like NumPy, but that would take forever and not scale well. So we use libraries like PyTorch to abstract away these details, and use prebuilt tools for designing networks and training them without too much code.\n",
    "\n",
    "We will use Colab for this tutorial since the GPUs are free.\n",
    "\n",
    "**We will assume proficiency with basic python and numpy** (see [here](https://colab.research.google.com/github/cs231n/cs231n.github.io/blob/master/python-colab.ipynb#scrollTo=Numpy) for a basic introduction to numpy and its syntax... and some basic python review).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuQhjQNjaLg0"
   },
   "source": [
    "### What is PyTorch?\n",
    "\n",
    "PyTorch works exactly like numpy but it also keeps track, on the fly, of all the operations we perform with Tensor objects (which behave like ndarrays). It caches everything it needs to calculate arbitrary derivatives of any scalar function with respect to any element of any tensor that was used in the computation for that scalar function (in other words, you will be able to calculate the derivative of anything with respect to anything). This is an enormous step up from the dinosaur days of having to calculate and plug in your own derivatives to layers of neural networks.\n",
    "\n",
    "### Why?\n",
    "\n",
    "* Small code footprint, but incredibly powerful\n",
    "* Our code will now run on GPUs without us having to touch CUDA ourselves directly\n",
    "* PyTorch is used everywhere in research and in industry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMhHaMi0aLg2"
   },
   "source": [
    "## How will I learn PyTorch?\n",
    "\n",
    "PyTorch has an excellent series of tutorials [here](https://pytorch.org/tutorials/).\n",
    "\n",
    "Justin Johnson has made an excellent [tutorial](https://github.com/jcjohnson/pytorch-examples) for PyTorch.\n",
    "\n",
    "You can also find the detailed [API doc](http://pytorch.org/docs/stable/index.html) here. In general the PyTorch documentation is incredibly comprehensive and should be referenced / searched wherever possible if you have a question on how some function works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBWip2PFaLg3"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "This assignment has 6 parts. We will start by talking about PyTorch and its similarities to numpy. We will then move on to talking about PyTorch's automatic differentiation engine. We will quickly go over APIs and function calls for loading datasets. Finally, we will show 3 different levels of abstraction that can be used for creating neural networks.\n",
    "\n",
    "1. Numpy / Torch\n",
    "2. Automatic Differentiation\n",
    "3. Datasets / Dataloaders / Augmentations\n",
    "4. Barebones PyTorch (Low level network design)\n",
    "5. nn.Module PyTorch (Middle level network design)\n",
    "6. nn.Sequential PyTorch (High level network design)\n",
    "\n",
    "Here is a table of comparison for each abstraction level:\n",
    "\n",
    "| API           | Flexibility | Convenience |\n",
    "|---------------|-------------|-------------|\n",
    "| Barebone      | High        | Low         |\n",
    "| `nn.Module`     | High        | Medium      |\n",
    "| `nn.Sequential` | Low         | High        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYGdKvrVmlhp"
   },
   "source": [
    "# 1. Numpy / Torch\n",
    "\n",
    "In this section we are going to show some of the similarities between numpy and pytorch.\n",
    "\n",
    "PyTorch is based around Tensor() objects. They behave exactly like numpy arrays in terms of what they are and what you can do with them. Just as basic ndarrays with floating point numbers are just higher dimensional generalizations of matrices, tensors are no different. These tensors are created as the returned value from ``torch.tensor(data)`` where ``data`` is a (optionally nested) list of integers or floats or booleans, or even ndarrays themselves!\n",
    "\n",
    "Like numpy, pytorch has a lot of mathmatical operations that can be performed on tensors... such as addition, elementwise multiplication, matrix multiplication, etc.\n",
    "\n",
    "This cannot be stressed enough, ignoring what's going on under the hood, tensors behave EXACTLY like ndarrays, so there is nothing to fear!\n",
    "\n",
    "This similarity is demonstrated again in [this](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) pytorch tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pS7ELwqlYtm0"
   },
   "source": [
    "## Numpy PyTorch Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kCLub9iRgNyC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjeCiMlGPeFG"
   },
   "source": [
    "The most basic way to create tensors is to just pass nested lists into a `torch.tensor()` function call. Here, we will create the 2x2 identity matrix from a nested list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FVOyr0eEO3Js",
    "outputId": "9928b65d-41d5-46ff-f549-49de05d0385c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix contents ------------------\n",
      "Numpy Array Contents and type:\n",
      "np array: \n",
      " [[1. 0.]\n",
      " [0. 1.]] \n",
      " <class 'numpy.ndarray'> \n",
      " (2, 2) \n",
      "\n",
      "Torch Tensor Contents and type:\n",
      "torch tensor: \n",
      " tensor([[1., 0.],\n",
      "        [0., 1.]]) \n",
      " <class 'numpy.ndarray'> \n",
      " torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "data_a = [[1.,0.], [0., 1.]]\n",
    "np_a = np.array(data_a)\n",
    "torch_a = torch.tensor(data_a)\n",
    "\n",
    "\n",
    "print('Matrix contents ------------------')\n",
    "print('Numpy Array Contents and type:')\n",
    "print('np array: \\n', np_a, '\\n', type(np_a), '\\n', np_a.shape, '\\n')\n",
    "print('Torch Tensor Contents and type:')\n",
    "print('torch tensor: \\n', torch_a, '\\n', type(np_a), '\\n', torch_a.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZ5mzIUWQQSn"
   },
   "source": [
    "We can see that even indexing into these objects works the same way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaQEZ_nPQP1R",
    "outputId": "b16a9d9d-0ec7-4c70-edd4-2a114694fab9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing at (i, j) = (1, 1)\n",
      "1.0 <class 'numpy.float64'>\n",
      "tensor(1.) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print('Indexing at (i, j) = (1, 1)')\n",
    "print(np_a[1, 1], type(np_a[1, 1]))\n",
    "print(torch_a[1, 1], type(torch_a[1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRnilQoMxKz-"
   },
   "source": [
    "We can reshape our arrays in similar ways too.\n",
    "\n",
    "PyTorch's [view](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html) is analogous to numpy's [reshape](https://numpy.org/doc/stable/reference/generated/numpy.reshape.html) method: it reshapes a tensor's dimensions according to the arguments of the function.\n",
    "\n",
    "As an example, say we want to take a tensor of shape `2 x 3 x 3` and reshape it into a `2 x 9` tensor. The output tensor will have its elements laid out accordingly:\n",
    "\n",
    "```python\n",
    "input[0, 0, 0] == output[0, 0]\n",
    "input[0, 0, 1] == output[0, 1]\n",
    "input[0, 0, 2] == output[0, 2]\n",
    "# After this, we increment on the next dimension\n",
    "# of the input tensor since the last two dimensions\n",
    "# are only 3 long\n",
    "input[0, 1, 0] == output[0, 3]\n",
    "...\n",
    "input[0, 2, 2] == output[0, 8]\n",
    "input[1, 0, 0] == output[1, 0]\n",
    "...\n",
    "```\n",
    "\n",
    "Basically, if you were to iterate over every element in the tensors (before and after reshaping) starting with the last dimensions first, the order of the elements printed is the same for both tensors.\n",
    "\n",
    "PyTorch additionally has another [reshape](https://pytorch.org/docs/stable/generated/torch.reshape.html) method available: the main difference between the two is that `.reshape` *might* copy the underlying data of a tensor into a new tensor (you can find a more comprehensive comparison [here](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8L8514pJxK4l",
    "outputId": "60c142a4-a93d-4a1a-d345-0c5012fbf97a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before reshaping:\n",
      "tensor([[[ 0.,  1.,  2.],\n",
      "         [ 3.,  4.,  5.],\n",
      "         [ 6.,  7.,  8.]],\n",
      "\n",
      "        [[ 9., 10., 11.],\n",
      "         [12., 13., 14.],\n",
      "         [15., 16., 17.]]])\n",
      "After reshaping\n",
      "tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.],\n",
      "        [ 9., 10., 11., 12., 13., 14., 15., 16., 17.]])\n",
      "\n",
      "tensor(0.) tensor(0.)\n",
      "tensor(1.) tensor(1.)\n",
      "tensor(2.) tensor(2.)\n",
      "tensor(3.) tensor(3.)\n",
      "...\n",
      "tensor(8.) tensor(8.)\n",
      "tensor(9.) tensor(9.)\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "start = torch.zeros((2, 3, 3))\n",
    "idx = 0\n",
    "for i in range(start.shape[0]):\n",
    "    for j in range(start.shape[1]):\n",
    "        for k in range(start.shape[2]):\n",
    "            start[i, j, k] = idx\n",
    "            idx += 1\n",
    "print(\"Before reshaping:\")\n",
    "print(start)\n",
    "end = start.view(2, 9)\n",
    "print(\"After reshaping\")\n",
    "print(end)\n",
    "print()\n",
    "\n",
    "print(start[0, 0, 0], end[0, 0])\n",
    "print(start[0, 0, 1], end[0, 1])\n",
    "print(start[0, 0, 2], end[0, 2])\n",
    "print(start[0, 1, 0], end[0, 3])\n",
    "print('...')\n",
    "print(start[0, 2, 2], end[0, 8])\n",
    "print(start[1, 0, 0], end[1, 0])\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uM6UezLiRYV_"
   },
   "source": [
    "We can see that numpy arrays and pytorch tensors have the same types of operations associated with them. They all support + (addition), - (subtraction), * (elementwise multiplication), @ (matrix multiplication where the operands are matrices), and even truth operations like == and >= (virtually, any syntax that works with numpy arrays will most likely work with pytorch tensors as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDKHw3ewRX1Q",
    "outputId": "e4ecda28-4024-428c-c572-e467fbeb247e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix a\n",
      " [[1. 0.]\n",
      " [0. 1.]] \n",
      "Matrix b\n",
      " [[1. 1.]\n",
      " [1. 1.]] \n",
      "\n",
      "----------------------\n",
      "Addition (a + b)\n",
      "[[2. 1.]\n",
      " [1. 2.]]\n",
      "tensor([[2., 1.],\n",
      "        [1., 2.]])\n",
      "\n",
      "----------------------\n",
      "Subtraction (a - b)\n",
      "[[ 0. -1.]\n",
      " [-1.  0.]]\n",
      "tensor([[ 0., -1.],\n",
      "        [-1.,  0.]])\n",
      "\n",
      "----------------------\n",
      "Elementwise Multiplication (a * b)\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "tensor([[1., 0.],\n",
      "        [0., 1.]])\n",
      "\n",
      "----------------------\n",
      "Matrix Multiplication (a @ b)\n",
      "[[1. 1.]\n",
      " [1. 1.]]\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_b = [[1., 1.], [1., 1.]]\n",
    "np_b = np.array(data_b)\n",
    "torch_b = torch.tensor(data_b)\n",
    "\n",
    "print('Matrix a\\n', np_a, '\\nMatrix b\\n', np_b, '\\n')\n",
    "\n",
    "print('----------------------')\n",
    "print('Addition (a + b)')\n",
    "print(np_a + np_b)\n",
    "print(torch_a + torch_b)\n",
    "print()\n",
    "\n",
    "print('----------------------')\n",
    "print('Subtraction (a - b)')\n",
    "print(np_a - np_b)\n",
    "print(torch_a - torch_b)\n",
    "print()\n",
    "\n",
    "print('----------------------')\n",
    "print('Elementwise Multiplication (a * b)')\n",
    "print(np_a * np_b)\n",
    "print(torch_a * torch_b)\n",
    "print()\n",
    "\n",
    "print('----------------------')\n",
    "print('Matrix Multiplication (a @ b)')\n",
    "print(np_a @ np_b)\n",
    "print(torch_a @ torch_b)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEK1V2hoVo7y"
   },
   "source": [
    "PyTorch even supports some of the linear algebra operations that numpy supports! Not all of them share the same function names, but the ones below do. Here, we only show the functions for calculating norms and inverses, but just like numpy, torch has functionality for everything from QR decompositions to SVD to solving systems of linear equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h5b9zHxISl2U",
    "outputId": "84b4be40-2436-46ef-c8f7-07b17bcfc34b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm -------------------------\n",
      "1.0\n",
      "tensor(1.)\n",
      "\n",
      "Inverse ---------------------\n",
      "[[ 1.6 -1.2]\n",
      " [ 1.2  1.6]]\n",
      "tensor([[ 1.6000, -1.2000],\n",
      "        [ 1.2000,  1.6000]])\n"
     ]
    }
   ],
   "source": [
    "print('Norm -------------------------')\n",
    "data_v = [0.6, 0.8]\n",
    "np_v = np.array(data_v)\n",
    "torch_v = torch.tensor(data_v)\n",
    "print(np.linalg.norm(np_v))\n",
    "print(torch.linalg.norm(torch_v))\n",
    "print()\n",
    "\n",
    "print('Inverse ---------------------')\n",
    "data_M = [[0.4, 0.3], [-0.3, 0.4]]\n",
    "np_M = np.array(data_M)\n",
    "torch_M = torch.tensor(data_M)\n",
    "print(np.linalg.inv(np_M))\n",
    "print(torch.linalg.inv(torch_M))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XT4OrIigSNY"
   },
   "source": [
    "Finally, we can also convert back and forth between tensors and ndarrays. We can call ``torch.tensor()`` on an ndarray to receive it as tensor, or call ``.numpy()`` on a tensor to get the numpy version of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qeJEetQAgSWn",
    "outputId": "bb5341a6-b86d-476b-a46c-b28f43bcca73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0.],\n",
      "        [0., 1.]], dtype=torch.float64)\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(np_a))\n",
    "print(torch_a.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qE3x7fHVaLg6"
   },
   "source": [
    "## Devices\n",
    "\n",
    "Here is one of the major differences between pytorch and numpy: GPUs. In the code block below, we will define a \"device\" as the object returned by calling ``torch.device('cuda')`` or ``torch.device('cpu')``. These objects can be then passed into new tensors that we create, indicating what hardware device they should be stored on. You can also move around existing tensors across devices by using the ``.to()`` function: calling ``x.to(device)`` on tensor ``x`` will move it to the specified device.\n",
    "\n",
    "You have an option to **use the GPU by setting the ``USE_GPU`` flag to True below**. It is recommended, but not necessary to use a GPU for this assignment. Note that if your computer does not have CUDA enabled, `torch.cuda.is_available()` will return False and this notebook will fall back to CPU mode. The global variables `dtype` and `device` will control the data types throughout this assignment.\n",
    "\n",
    "If you are using Colab, you need to manually switch to a GPU device. You can do this by clicking `Runtime -> Change runtime type` and selecting `GPU` under `Hardware Accelerator`. Note that you have to rerun the cells from the top since the kernel gets restarted upon switching runtimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "itSfS4pUaLg6",
    "outputId": "1b4c9a22-7f19-493a-9f5a-e5d5137dfa76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "\n",
    "dtype = torch.float32 # we will use the float32 data type throughout this tutorial\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print('using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jBGwN25LDXh"
   },
   "source": [
    "Here, we see a tensor created with a device from the get go, and another example where we send an existing tensor to a device. If you indeed do have a GPU enabled, you should see ``'cuda:0'`` listed as the device type for these tensors. **The last example shouldn't work because sending a tensor to a device with a ``.to()`` call returns the a NEW object that is stored on the new device... the ``.to()`` operation isn't an 'in place' operation!**\n",
    "\n",
    "You will be using the ``.to()`` function on networks and tensors down the line to send entire neural networks and batches of data to a GPU during training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x1oatniBKvVg",
    "outputId": "822edeb7-20a0-4283-d32c-edb8f464febd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.9457,  0.1832],\n",
      "        [ 0.7670, -0.2213]], device='cuda:0') cuda:0\n",
      "tensor([[1.3666, 1.5648],\n",
      "        [0.7522, 0.2662]], device='cuda:0') cuda:0\n",
      "tensor([[ 1.0496,  0.9860],\n",
      "        [-0.4230, -0.3700]]) cpu\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn((2, 2), device = device)\n",
    "print(a, a.device)\n",
    "\n",
    "a = torch.randn((2, 2))\n",
    "a = a.to(device)\n",
    "print(a, a.device)\n",
    "\n",
    "a = torch.randn((2, 2))\n",
    "a.to(device)\n",
    "print(a, a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3mYkGAFMd9e"
   },
   "source": [
    "The following cell will error if your device is a GPU. This is because tensors not on the same device cannot be operated on together. This makes sense, because how on earth are you going to perform an arithmetic operation on tensors that are on separate hardware?\n",
    "\n",
    "Let this be a cautionary tale down the line when you train your networks: if your network is on a GPU, make sure you send your batches of data to the GPU as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "id": "O-AAIjPfMeBh",
    "outputId": "3e605e6f-a3cf-42cf-bfba-5ade2fd604f1"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51165/2847289752.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "torch.randn((2, 2), device = device) + torch.randn((2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVfslpsymuSZ"
   },
   "source": [
    "# 2.  PyTorch Autograd\n",
    "\n",
    "We have shown that, on the surface, torch tensors have the same functionality as numpy arrays, but under the hood, there is so much more going on! **We can specify that we want to take the derivatives of a tensor output with respect to the elements of a different tensor, when evaluated at specific points.** While torch won't be spitting out symbolic derivatives like ``f(x, w) = xw^2 ==> df/dw = 2wx``, it will evaluate ``df/dw`` at a specific value of ``x`` for a specific value of ``w``, which is all that we really need for deep learning.\n",
    "\n",
    "**Please note that the only syntactical thing you should come away from this portion with is that ``x.backward()`` will calculate the partial derivatives of every tensor with respect to ``x``.** You will never have to go in and look at what a tensor's ``.grad`` attribute is... we will using something called Optimizers later on that will handle the gradient updates for us automatically. The ``.float()`` casting (which, as the name suggests, converts a tensor to the float data type) is probably useful to know as well.\n",
    "\n",
    "For a specific pytorch tutorial on this subject of automatic differentiation, see [here](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kEzLzzdCfnY9"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "np0TgaDUiDbk"
   },
   "source": [
    "Here we will start with a basic function, ``f(x, w) = xw^2``, as given in the example above. We will first set our value for ``w`` that we want to evaluate the derivative of ``f`` at.\n",
    "\n",
    "Since we need to take the derivative of this function with respect to its inputs, we need a way to specify to torch to allow differentiation with respect to those inputs. For every tensor with respect to which we want to be able to differentiate a function with, we will set its ``.requires_grad`` field to True. This will ensure that after a computational graph is built and backpropagation has ran, those tensors will have an additional attribute storing their gradients!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "id": "qIkeGObKiDvR",
    "outputId": "7ec119fd-aaf0-4fdb-ca1a-f3be99216d3d"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "only Tensors of floating point and complex dtype can require gradients",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51165/1669447914.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: only Tensors of floating point and complex dtype can require gradients"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(5)\n",
    "w.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QP2hEF4ZksVd"
   },
   "source": [
    "Oh no! This is to be expected. Torch is telling us that in order to be able to differentiate with respect to something, that thing has to be a floating point number. This makes sense, because the derivative only works in a continuous space and integers are discrete.\n",
    "\n",
    "This is an issue that will crop up and give you all kinds of trouble if you aren't careful, especially since we frequently load in images where pixel values are integers in the range [0 - 255].\n",
    "\n",
    "Just to be safe, it is often a good idea just to cast things if you aren't certain what their type is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vdmWhKCniD5R"
   },
   "outputs": [],
   "source": [
    "w = w.float()\n",
    "w.requires_grad = True\n",
    "\n",
    "x = torch.tensor(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h3Pjcl1emS-F"
   },
   "source": [
    "Now we will evaluate `y = f(x, w)` with our tensors. The magical thing about pytorch is that we really only need to specify how to evaluate the function, and pytorch will take care of the rest for us! So, we will go ahead and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvgsutyZmSLV",
    "outputId": "367ce241-b4ab-4626-96a1-94ae477db3cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(100., grad_fn=<MulBackward0>) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "y = x * (w ** 2)\n",
    "print(y, type(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKCjEyLBmsYp"
   },
   "source": [
    "You can see that ``f(x, w)`` has been evaluated correctly, since `x = 4` and `w = 5`. We can also see a `grad_fn` field in the tensor. It's not important what this is, but it should be clear that pytorch is already ahead of the curve, and is stashing all the information it needs for actually calculating derivatives later, which happens with a call to `.backward()` function.\n",
    "\n",
    "The `a.backward()` function tells pytorch to go ahead and take the derivatives of `a` with respect to all the tensors that were used to create `a`. The outputs will be stashed in those tensors' `.grad` fields.\n",
    "\n",
    "More generally, in the context of backpropagating for neural networks, **the `a.backward()` method only makes sense if the tensor `a` contains only a single scalar as opposed to a vector or matrix. Otherwise you are computing the derivative of a vector or matrix with respect to a scalar weight, which will yield another vector / matrix instead of a scalar.** Since the components of our weights are scalars, we want the derivatives of our loss with respect to those components to be scalars as well (so that we can perform a gradient descent step). This is why, when performing batched gradient descent, you will average the loss across training examples, yielding a single scalar loss for that batch that we can backpropagate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yOEX5qEsnnjM",
    "outputId": "bd05f2d7-d8bd-4a45-b2a0-27fa488ff353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(40.)\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VEiIznop2-q"
   },
   "source": [
    "Yay! This is what we should expect since ``df/dw = 2wx = 2(5)(4)`` when `w = 5` and `x = 4`. Let's see what happens if we run the same cell again but with a different value of `w`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bYjyuvZrp-N",
    "outputId": "14a86aa6-4217-407e-bfcc-42b445e8faf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36., grad_fn=<MulBackward0>)\n",
      "tensor(64.)\n"
     ]
    }
   ],
   "source": [
    "w.data = torch.tensor(3.)\n",
    "y = x * (w ** 2)\n",
    "print(y)\n",
    "y.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0Ns76-vsMRC"
   },
   "source": [
    "What happened?!?!?! We can see that `y = f(4, 3) = 36.` evaluated correctly. This means that `df/dw` should have been `2(4)(3) = 24.` but we are seeing a `64.` in `w.grad` instead of a `24.`?\n",
    "\n",
    "**Turns out that when pytorch computes the derivative of `y` with respect to `w`, it doesn't just overwrite the `.grad` attribute of `w`, but actually adds to it in place.** Our `.grad` attribute was initially `40.`, and when we performed another backward pass, the new derivative was `24.`: instead of overwriting to the new gradient, pytorch added to the previous `w.grad`\n",
    "field. **This is why, after each batch of data you backpropagate on, you will need to clear your model's gradients.** We will tell you how to exactly do this later on, but let this be a cautionary tale to clear your gradients between gradient steps nonetheless.\n",
    "\n",
    "This is something that is useful when performing \"gradient accumulation\" (out of scope for now, but something worth googling eventually)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAyfyUgurqvG"
   },
   "source": [
    "More generally, we can actually create any function `f` that takes in arbitrary parameters, and still backpropagate from the output of the function to the inputs. Let's illustrate by creating a function called `f` which is our quadratic from earlier, but know that you can truly create any function so long as the operations inside are differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXZ0v-hRp2PP",
    "outputId": "2cb9625b-1b5e-4ba4-b4a3-486973d6cb3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24.)\n"
     ]
    }
   ],
   "source": [
    "# we should zero out the w.grad attribute to ensure we see the correct gradient this time\n",
    "w.grad.zero_()\n",
    "\n",
    "def f(w, x):\n",
    "    return x * (w ** 2)\n",
    "\n",
    "\n",
    "y = f(w, x)\n",
    "y.backward()\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDCI_P8Mvxo5"
   },
   "source": [
    "At this point, you can use pytorch to calculate arbitrary derivatives evaluated at arbitrary values.\n",
    "\n",
    "PyTorch has a [nice tutorial](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html) that shows a more complex example with nice diagrams, and states in greater detail about how it actually keeps track of different tensor computations, and we recommend you check it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V97omoMxaLg5"
   },
   "source": [
    "# 3. Datasets and Dataloading\n",
    "\n",
    "One of the critical parts of training neural networks is dealing with data and knowing how to augment / transform it.\n",
    "\n",
    "We will be using the `torch.utils.data` for the core of our dataloading pipeline, using it to create datasets and dataloaders. Moreover, the library `torchvision` (a subset of pytorch that contains utilities useful for computer vision tasks) will also be used for performing data augmentations.\n",
    "\n",
    "The official torch intro to datasets and dataloaders is [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) and the list of transforms (augmentations) in torchvision is available [here](https://pytorch.org/vision/stable/transforms.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "abAU242syW-s"
   },
   "outputs": [],
   "source": [
    "# Some misc imports used for demonstrations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# What we actually need for this to work\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aw9DIYO3pY-6"
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0ha4WxA4yU0"
   },
   "source": [
    "A dataset in PyTorch is any class inheriting from the `torch.utils.data.Dataset` parent class, with the following 3 methods:\n",
    "1. ` __init___(self, ...):`\n",
    "This is the initialization function where you can pass in whatever arguments you want for your custom dataset. Save all the information from the arguments in instance variables here so it will be easy to fetch them later in the methods given below.\n",
    "2. ` __len__(self):`\n",
    "Here, you simply need to return the length of your dataset (ie: the number of samples in the datasets).\n",
    "3. ` __getitem__(self, idx):`\n",
    "This is the meat of the dataset class. This is a function that must take in `self` and `idx` as arguments, but nothing else. Here, `idx` will be an integer between 0 (inclusive) and the length of your dataset (exclusive). You should be returning the tensor(s) corresponding to the `idx`-th training example (transformed / augmented if you so desire), and any other data you may want (like a label or any relevant metadata). Most of the time, for image data, this involves: opening an image file -> converting it to a tensor -> applying any augmentations -> outputting the final image and its corresponding label.\n",
    "\n",
    "Below is what your dataset will look like written out in skeleton code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "_5VpiNxYzUF_"
   },
   "outputs": [],
   "source": [
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, **kwargs):\n",
    "        pass # Insert your code here to initialize the dataset and load up what you need\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass # Figure out how to load up and return the idx-th item in the dataset\n",
    "        return x, label, other_misc_information_you_may_want\n",
    "\n",
    "    def __len__(self):\n",
    "        pass # Return the length of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQWNz5WT9Fqv"
   },
   "source": [
    "Let's make a toy dataset where the `(x, y)` pairs are `(x, sin(x))` respectively... i.e., our dataset is just inputs and outputs of the sine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yjUxyUHmzUPY"
   },
   "outputs": [],
   "source": [
    "class sin_dataset(Dataset):\n",
    "    def __init__(self, num_examples, x_range):\n",
    "        \"\"\"\n",
    "        Documentation here:\n",
    "        num_examples is the number of examples we want this dataset to have\n",
    "        x_range is a tuple of (lowest_possible_x, highest_possible_x)\n",
    "        \"\"\"\n",
    "        self.num_examples = num_examples\n",
    "        # torch.rand(shape) outputs a tensor with shape `shape` whose entries are uniformly distributed in the range [0, 1)\n",
    "        self.x_vals = (torch.rand(num_examples) * (x_range[1] - x_range[0])) + x_range[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_vals[idx], torch.sin(self.x_vals[idx])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CeDaUWsoAKaX"
   },
   "source": [
    "We can index into this dataset directly once we instantiate a sin_dataset object, and can even iterate over it!\n",
    "\n",
    "This creates a super convenient way to pass data around, adding a nice structured abstraction to the process of getting and loading in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "cOEwXZprAUsi",
    "outputId": "c6cd5064-23c5-43a4-f18d-d1acde422f2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th entry (tensor(1.6187), tensor(0.9989))\n",
      "1-th entry (tensor(-1.3866), tensor(-0.9831))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(1)  # Setting the seed just for reproduceability... can remove this if you want\n",
    "ds = sin_dataset(100, (-torch.pi, torch.pi))\n",
    "\n",
    "print('0-th entry', ds[0])\n",
    "print('1-th entry', ds[1])\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "for x, y in ds:\n",
    "    xs.append(x)\n",
    "    ys.append(y)\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWkPUL8LpUS6"
   },
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1-TSpwDBWvp"
   },
   "source": [
    "Let's now learn about dataloaders. They simply wrap our dataset objects and batch our data for us, even shuffling it if we so want (we usually do!). They use multiprocessing under the hood to fetch our data quickly, providing one further abstraction away from the data loading process by allowing us to now directly request batches of data.\n",
    "\n",
    "PyTorch's base `DataLoader()` class will automatically batch things for us. If our dataset spits out 4 tensors in the `getitem(self, idx)` function, then the dataloader with spit out (in the same order) 4 batched tensors of, where an extra dimension (the batch dimension) is added as the first dimension.\n",
    "\n",
    "In the case of visual data, if your images are of size `C x H x W` (channels in the image, width, height), the dataloader will spit out a batch tensor of size `N x C x H x W` where `N` is the number of samples in the batch and `batch[i]` is the `i`th training example (since we indexed along the first dimension, which is the batch dimension)\n",
    "\n",
    "We will not be covering ways to create your own dataloader since, so long as you are returning tensors, no matter how many, pytorch will almost always get the batching right. For more complex data, you may have to define your own batching functions (but this is typically more commonly done in NLP than CV).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGfoZQXMCe7c",
    "outputId": "ae65566c-337a-4789-c106-e1eca347e81f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched x tensor([ 1.6187, -1.3866, -0.6090,  1.4746, -2.9576,  1.8841, -0.6463,  1.5983,\n",
      "         0.4367, -0.3847])\n",
      "Batched y tensor([ 0.9989, -0.9831, -0.5721,  0.9954, -0.1829,  0.9513, -0.6022,  0.9996,\n",
      "         0.4230, -0.3753])\n",
      "Example 0      x:  tensor(1.6187)     y:  tensor(0.9989)\n",
      "Example 1      x:  tensor(-1.3866)     y:  tensor(-0.9831)\n",
      "Example 2      x:  tensor(-0.6090)     y:  tensor(-0.5721)\n",
      "Example 3      x:  tensor(1.4746)     y:  tensor(0.9954)\n",
      "Example 4      x:  tensor(-2.9576)     y:  tensor(-0.1829)\n",
      "Example 5      x:  tensor(1.8841)     y:  tensor(0.9513)\n",
      "Example 6      x:  tensor(-0.6463)     y:  tensor(-0.6022)\n",
      "Example 7      x:  tensor(1.5983)     y:  tensor(0.9996)\n",
      "Example 8      x:  tensor(0.4367)     y:  tensor(0.4230)\n",
      "Example 9      x:  tensor(-0.3847)     y:  tensor(-0.3753)\n"
     ]
    }
   ],
   "source": [
    "# we want each batch to contain 10 samples\n",
    "# since we set shuffle=False, the first batch will contain the first 10 samples (idx in range(0, 10)) of the dataset,\n",
    "# the next batch will contain the next 10 samples (idx in range(10, 20)) and so on\n",
    "loader = DataLoader(ds, batch_size=10, shuffle=False)\n",
    "\n",
    "batch_x, batch_y = next(iter(loader))\n",
    "print('Batched x', batch_x)\n",
    "print('Batched y', batch_y)\n",
    "\n",
    "\n",
    "for i, (x, y) in enumerate(ds):\n",
    "    if i >= 10: # We only want the first 10\n",
    "        break\n",
    "    print('Example ' + str(i) + '     ', 'x: ', x, '    y: ', y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxeylqHkG-bx"
   },
   "source": [
    "What we did here is create an iterable (see [here](https://www.w3schools.com/python/python_iterators.asp) for a tutorial on python iterators) from our dataloader by calling `iter(loader)`. We retrieved its first element by calling `next()`, which spat out 2 tensors: a batched version of the first 10 `x` values and a batched version of the first 10 `y` values. The order is the same and the values are themselves unchanged as we can confirm by printing out the first 10 elements from the dataset by hand.\n",
    "\n",
    "In your training loops, you will iterate over your dataloader by doing something like this:\n",
    "\n",
    "```python\n",
    "for batch in train_loader:\n",
    "    x, label = batch\n",
    "    # Insert one step of gradient descent here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBkxHGmrpM23"
   },
   "source": [
    "## Image Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWQJmQeQKTB3"
   },
   "source": [
    "One last thing you should know (before we load up a real dataset and move on) is how to transform / augment images. For this, we will usually use the transforms defined in `torchvision.transforms` (though, depending on the problem you are working on, you may have to define custom transformations). These will be objects that are callable on some data, returning the transformed version of said data.\n",
    "\n",
    "Below we have an example of a basic transform: the `ToTensor()` transform. This is an object that will take in numpy arrays or PIL (another format for representing image files in python) images and spit out their tensor version.\n",
    "Something to be cautious about: PIL or numpy images are represented with shapes `H x W x C` and `ToTensor()` will automatically transpose them to a shape of `C x H x W`, which is what pytorch prefers. Moreover, PIL or numpy images also tend to be integers in the range `[0, 255]` (each pixel is 1 bit), and `ToTensor()` scales them down to floats in the range `[0., 1.]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJ5REZ5nMLSc",
    "outputId": "2ff9d168-5128-46c0-93dd-5abf1353c0c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the original np array\n",
      "<class 'numpy.ndarray'>\n",
      "[[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15]\n",
      " [ 16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31]\n",
      " [ 32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47]\n",
      " [ 48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63]\n",
      " [ 64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79]\n",
      " [ 80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95]\n",
      " [ 96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111]\n",
      " [112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127]\n",
      " [128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143]\n",
      " [144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159]\n",
      " [160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175]\n",
      " [176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191]\n",
      " [192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207]\n",
      " [208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223]\n",
      " [224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239]\n",
      " [240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255]] \n",
      "\n",
      "\n",
      "This is the transformed array\n",
      "<class 'torch.Tensor'>\n",
      "tensor([[[0.0000, 0.0039, 0.0078, 0.0118, 0.0157, 0.0196, 0.0235, 0.0275,\n",
      "          0.0314, 0.0353, 0.0392, 0.0431, 0.0471, 0.0510, 0.0549, 0.0588],\n",
      "         [0.0627, 0.0667, 0.0706, 0.0745, 0.0784, 0.0824, 0.0863, 0.0902,\n",
      "          0.0941, 0.0980, 0.1020, 0.1059, 0.1098, 0.1137, 0.1176, 0.1216],\n",
      "         [0.1255, 0.1294, 0.1333, 0.1373, 0.1412, 0.1451, 0.1490, 0.1529,\n",
      "          0.1569, 0.1608, 0.1647, 0.1686, 0.1725, 0.1765, 0.1804, 0.1843],\n",
      "         [0.1882, 0.1922, 0.1961, 0.2000, 0.2039, 0.2078, 0.2118, 0.2157,\n",
      "          0.2196, 0.2235, 0.2275, 0.2314, 0.2353, 0.2392, 0.2431, 0.2471],\n",
      "         [0.2510, 0.2549, 0.2588, 0.2627, 0.2667, 0.2706, 0.2745, 0.2784,\n",
      "          0.2824, 0.2863, 0.2902, 0.2941, 0.2980, 0.3020, 0.3059, 0.3098],\n",
      "         [0.3137, 0.3176, 0.3216, 0.3255, 0.3294, 0.3333, 0.3373, 0.3412,\n",
      "          0.3451, 0.3490, 0.3529, 0.3569, 0.3608, 0.3647, 0.3686, 0.3725],\n",
      "         [0.3765, 0.3804, 0.3843, 0.3882, 0.3922, 0.3961, 0.4000, 0.4039,\n",
      "          0.4078, 0.4118, 0.4157, 0.4196, 0.4235, 0.4275, 0.4314, 0.4353],\n",
      "         [0.4392, 0.4431, 0.4471, 0.4510, 0.4549, 0.4588, 0.4627, 0.4667,\n",
      "          0.4706, 0.4745, 0.4784, 0.4824, 0.4863, 0.4902, 0.4941, 0.4980],\n",
      "         [0.5020, 0.5059, 0.5098, 0.5137, 0.5176, 0.5216, 0.5255, 0.5294,\n",
      "          0.5333, 0.5373, 0.5412, 0.5451, 0.5490, 0.5529, 0.5569, 0.5608],\n",
      "         [0.5647, 0.5686, 0.5725, 0.5765, 0.5804, 0.5843, 0.5882, 0.5922,\n",
      "          0.5961, 0.6000, 0.6039, 0.6078, 0.6118, 0.6157, 0.6196, 0.6235],\n",
      "         [0.6275, 0.6314, 0.6353, 0.6392, 0.6431, 0.6471, 0.6510, 0.6549,\n",
      "          0.6588, 0.6627, 0.6667, 0.6706, 0.6745, 0.6784, 0.6824, 0.6863],\n",
      "         [0.6902, 0.6941, 0.6980, 0.7020, 0.7059, 0.7098, 0.7137, 0.7176,\n",
      "          0.7216, 0.7255, 0.7294, 0.7333, 0.7373, 0.7412, 0.7451, 0.7490],\n",
      "         [0.7529, 0.7569, 0.7608, 0.7647, 0.7686, 0.7725, 0.7765, 0.7804,\n",
      "          0.7843, 0.7882, 0.7922, 0.7961, 0.8000, 0.8039, 0.8078, 0.8118],\n",
      "         [0.8157, 0.8196, 0.8235, 0.8275, 0.8314, 0.8353, 0.8392, 0.8431,\n",
      "          0.8471, 0.8510, 0.8549, 0.8588, 0.8627, 0.8667, 0.8706, 0.8745],\n",
      "         [0.8784, 0.8824, 0.8863, 0.8902, 0.8941, 0.8980, 0.9020, 0.9059,\n",
      "          0.9098, 0.9137, 0.9176, 0.9216, 0.9255, 0.9294, 0.9333, 0.9373],\n",
      "         [0.9412, 0.9451, 0.9490, 0.9529, 0.9569, 0.9608, 0.9647, 0.9686,\n",
      "          0.9725, 0.9765, 0.9804, 0.9843, 0.9882, 0.9922, 0.9961, 1.0000]]])\n"
     ]
    }
   ],
   "source": [
    "transform = T.ToTensor()\n",
    "\n",
    "test = np.arange(256).reshape(16, 16).astype(np.uint8)\n",
    "print('This is the original np array')\n",
    "print(type(test))\n",
    "print(test, '\\n\\n')\n",
    "\n",
    "transformed = transform(test)\n",
    "print('This is the transformed array')\n",
    "print(type(transformed))\n",
    "print(transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NIAOxCqjzoK"
   },
   "source": [
    "You can see what we did and how this worked. We take the instantiated transform object, and then essentially treat it as a function by calling it on the data we want to transform. A little off the wall, but also pretty straightforward. We can do this with a whole host of transforms in the torchvision library found [here](https://pytorch.org/vision/stable/transforms.html) (one needs to be careful because some of the transforms only accept numpy arrays or PIL images as inputs, while other transforms only accept tensor objects as inputs). We will compose them in the example shown below, using `transforms.Compose`. For computer vision tasks, it is very useful to define image augmentations using the `torchvision.transforms` class in this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "dCpcaR84ka18",
    "outputId": "582b11d0-c4e3-4c38-a8eb-298aa8af6b53"
   },
   "outputs": [],
   "source": [
    "# This gets us a dummy image to use\n",
    "!wget -O image.jpg https://upload.wikimedia.org/wikipedia/commons/c/c5/Peacock_Plumage.jpg\n",
    "\n",
    "\n",
    "img_as_np = plt.imread('image.jpg')\n",
    "\n",
    "print('\\n\\n\\nThis is our starting image')\n",
    "plt.imshow(img_as_np)\n",
    "plt.show()\n",
    "\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Resize((400, 400)),\n",
    "                T.RandomRotation(degrees=20)\n",
    "            ])\n",
    "\n",
    "output_tensor = transform(img_as_np)\n",
    "\n",
    "plt.imshow(output_tensor.permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NqbLPoIoN11"
   },
   "source": [
    "Here, you can see the starting image, and the image after it has been passed through our transform (which is then subsequently put back as a numpy array). We first turned it into a tensor (with `ToTensor()`), then resized it to spatial dimensions `400 x 400` (with `Resize((400, 400))`), and then finally rotated it by some random angle between (-20, 20) (with `RandomRotation(degrees=20)`).\n",
    "\n",
    "--Aside--\n",
    "\n",
    "You may notice that we needed a `output_tensor.permute(1, 2, 0)` before we converted the result into a numpy array. As mentioned earlier, in PyTorch, it is the convention to have the channel dimension of an image before the spatial dimensions (the `C x H x W` format), whereas in matplotlib, PIL, and a few other vision libraries, it is a convention to have it at the end (the `H x W x C` format).\n",
    "So we needed to \"permute\" or \"transpose\" the tensor appropriately to reorder its dimensions before plotting it with matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RDqFr1sc8Xj"
   },
   "source": [
    "One last aside: We won't cover it in this specific notebook but, for your own reading, there is a fantastic library of computer vision transforms called Albumentations [here](https://albumentations.ai/docs/getting_started/transforms_and_targets/) which is one of the nicest libraries for image transforms, especially when you need to randomly augment an image AND its corresponding labels (such as keypoints or segmentation masks). For basic classification tasks, where augmenting our data shouldn't change the prediction, the built-in torchvision transforms will be sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6lLn85vpIvV"
   },
   "source": [
    "## Prefab Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99tiovrYKnw1"
   },
   "source": [
    "We also have lots of premade, publicly available datasets that we can fetch from torchvision (see [here](https://pytorch.org/vision/stable/datasets.html) for a full list). Some of these allow us to download the data directly but, in other cases like with ImageNet, you will have to supply the data yourself. Below, we are going to use this to download the `CIFAR-10` dataset.\n",
    "\n",
    "As a note: we are using computer vision datasets here, but in addition to torchvision, PyTorch also has libraries like `torchaudio` and `torchtext`, so depending on your speciality, you may want to check out those specific libraries in depth on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yljyb8U_aLg6",
    "outputId": "8fee679a-a94a-4b87-e1fa-01174ca3c088"
   },
   "outputs": [],
   "source": [
    "NUM_TRAIN = 49000\n",
    "\n",
    "# Here we set up a transform to preprocess the data by normalizing its RGB\n",
    "# values; we've hardcoded the mean and standard deviation ourselves.\n",
    "# If we want to add data augmentations, we can append transformations from\n",
    "# torchvision to the Compose object below. However, we would need to create\n",
    "# two sets of transformations: one for the training dataloader, and one for\n",
    "# the test / validation dataloader.\n",
    "\n",
    "transform = T.Compose([\n",
    "                T.ToTensor(),\n",
    "                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "\n",
    "# We set up a Dataset object for each split (train / val / test); Datasets load\n",
    "# training examples one at a time, so we wrap each Dataset in a DataLoader which\n",
    "# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n",
    "# training set into train and val sets by passing a Sampler object to the\n",
    "# DataLoader, which defines how it should sample from the underlying Dataset.\n",
    "# You could also just take subsets of the dataset and pass them into separate\n",
    "# dataloaders in order to create your own train and test splits\n",
    "cifar10_train = datasets.CIFAR10('./datasets', train=True, download=True,\n",
    "                                 transform=transform)\n",
    "loader_train = DataLoader(cifar10_train, batch_size=64,\n",
    "                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10('./datasets', train=True, download=True,\n",
    "                               transform=transform)\n",
    "loader_val = DataLoader(cifar10_val, batch_size=64,\n",
    "                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n",
    "\n",
    "cifar10_test = datasets.CIFAR10('./datasets', train=False, download=True,\n",
    "                                transform=transform)\n",
    "loader_test = DataLoader(cifar10_test, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTekK6KWY5cU"
   },
   "source": [
    "Let's verify the shape of the tensors returned in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFZ7AKc7Y-O6",
    "outputId": "3966666a-d179-4200-80db-32bfff5a3a50"
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(loader_train))\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0X2l_ntZF6K"
   },
   "source": [
    "The 64 at the beginning makes sense since we specified it as a batch size above. The remaining 3 numbers indicate that each image in the CIFAR10 dataset is `3 x 32 x 32`, i.e., a `32 x 32` RGB image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSYDIbkAjB7o",
    "outputId": "a7d5699a-2917-48ba-c67f-4621f0ae40fc"
   },
   "outputs": [],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spqlsY9JaLg8"
   },
   "source": [
    "# 4. Barebones PyTorch\n",
    "\n",
    "Now that you know how torch functions on a basic level, have seen it's automatic differentiation engine, and its dataloading utilities, all that's left to learn is a way to build and train neural networks!\n",
    "\n",
    "PyTorch ships with high-level APIs to help us define model architectures conveniently, which we will cover in the next section of this tutorial. In this section, we will start with the barebone PyTorch elements to understand the autograd engine better. After this exercise, you will come to appreciate the high-level model API more.\n",
    "\n",
    "We will start with a simple fully-connected ReLU network with two hidden layers and no biases for CIFAR classification.\n",
    "This implementation computes the forward pass using operations on PyTorch Tensors, and uses PyTorch autograd to compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQnwB3DrNK3N"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F  # useful stateless functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqIGRx_VaLg9"
   },
   "source": [
    "## PyTorch Tensors: Flatten Function\n",
    "The flatten function, like its name suggests, simply flattens a multidimensional tensor into one with fewer dimensions. This useful for converting batches of images of shape `(N, C, H, W)` into 2D batches of shape `(N, C * H * W)`, i.e., compressing each image down into a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lYf-9H_LaLg9",
    "outputId": "fba4423d-e254-423a-d6c7-7ccbed8d2c68"
   },
   "outputs": [],
   "source": [
    "def flatten(x):\n",
    "    N = x.shape[0] # read in N, C, H, W\n",
    "    return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "\n",
    "x = torch.arange(12).view(2, 1, 3, 2)\n",
    "print('Before flattening: \\n', x)\n",
    "print('After flattening: \\n', flatten(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qZThopiaLg-"
   },
   "source": [
    "## Barebones PyTorch: Two-Layer Network\n",
    "\n",
    "Here we define a function `two_layer_fc` which performs the forward pass of a two-layer fully-connected ReLU network on a batch of image data. After defining the forward pass we check that it doesn't crash and that it produces outputs of the right shape by running zeros through the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULeCnkTEaLg-",
    "outputId": "a14f4bff-6e18-44ea-b80b-cb419f8a4e6b"
   },
   "outputs": [],
   "source": [
    "def two_layer_fc(x, params):\n",
    "    \"\"\"\n",
    "    A fully-connected neural networks; the architecture is:\n",
    "    fully connected layer -> ReLU -> fully connected layer.\n",
    "\n",
    "    Note that this function only defines the forward pass;\n",
    "    PyTorch will take care of the backward pass for us.\n",
    "\n",
    "    The input to the network will be a minibatch of data, of shape\n",
    "    (N, d1, ..., dM) where d1 * ... * dM = D. The hidden layer will have H units,\n",
    "    and the output layer will produce scores for C classes.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A PyTorch Tensor of shape (N, d1, ..., dM), giving a minibatch of\n",
    "      input data.\n",
    "    - params: A list [w1, w2] of PyTorch Tensors giving weights for the network;\n",
    "      w1 has shape (D, H) and w2 has shape (H, C).\n",
    "\n",
    "    Returns:\n",
    "    - scores: A PyTorch Tensor of shape (N, C) giving classification scores for\n",
    "      the input data x.\n",
    "    \"\"\"\n",
    "    # first we flatten the image\n",
    "    x = flatten(x)  # shape: [batch_size, C x H x W]\n",
    "\n",
    "    w1, w2 = params\n",
    "\n",
    "    # Forward pass: compute predicted y using operations on Tensors. Since w1 and\n",
    "    # w2 have requires_grad=True, operations involving these Tensors will cause\n",
    "    # PyTorch to build a computational graph, allowing automatic computation of\n",
    "    # gradients. Since we are no longer implementing the backward pass by hand we\n",
    "    # don't need to keep references to intermediate values.\n",
    "    x = F.relu(x.mm(w1))\n",
    "    x = x.mm(w2)\n",
    "    return x\n",
    "\n",
    "\n",
    "hidden_layer_size = 42\n",
    "x = torch.zeros((64, 50), dtype=dtype)  # minibatch size 64, feature dimension 50\n",
    "w1 = torch.zeros((50, hidden_layer_size), dtype=dtype)\n",
    "w2 = torch.zeros((hidden_layer_size, 10), dtype=dtype)\n",
    "scores = two_layer_fc(x, [w1, w2])\n",
    "print(scores.size())  # you should see [64, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iZyMy3uaLhA"
   },
   "source": [
    "## Barebones PyTorch: Initialization\n",
    "Let's write a couple utility methods to initialize the weight matrices for our models.\n",
    "\n",
    "- `random_weight(shape)` initializes a weight tensor with the Kaiming normalization method.\n",
    "- `zero_weight(shape)` initializes a weight tensor with all zeros. Useful for instantiating bias parameters.\n",
    "\n",
    "The `random_weight` function uses the Kaiming normal initialization method, described in:\n",
    "\n",
    "He et al, *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification*, ICCV 2015, https://arxiv.org/abs/1502.01852\n",
    "\n",
    "Notice that we are setting the device of the instantiated tensors to be our device from earlier... this will allow execution on the GPU, but we will need to make sure that all the data we use is also on the GPU or it will error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8gKcEwJ6aLhA",
    "outputId": "78a23206-66d3-4239-c7ef-d2cde3943a0a"
   },
   "outputs": [],
   "source": [
    "def random_weight(shape):\n",
    "    \"\"\"\n",
    "    Create random Tensors for weights; setting requires_grad=True means that we\n",
    "    want to compute gradients for these Tensors during the backward pass.\n",
    "    We use Kaiming normalization: sqrt(2 / fan_in)\n",
    "    \"\"\"\n",
    "    if len(shape) == 2:  # FC weight\n",
    "        fan_in = shape[0]\n",
    "    else:\n",
    "        fan_in = np.prod(shape[1:]) # conv weight [out_channel, in_channel, kH, kW]\n",
    "    # randn is standard normal distribution generator.\n",
    "    w = torch.randn(shape, device=device, dtype=dtype) * np.sqrt(2. / fan_in)\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "def zero_weight(shape):\n",
    "    return torch.zeros(shape, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "# create a weight of shape [3 x 5]\n",
    "# you should see the type `torch.cuda.FloatTensor` if you use GPU.\n",
    "# Otherwise it should be `torch.FloatTensor`\n",
    "random_weight((3, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDCsM6PfaLhB"
   },
   "source": [
    "## Barebones PyTorch: Check Accuracy\n",
    "When training the model we will use the following function to check the accuracy of our model on the training or validation sets.\n",
    "\n",
    "When checking accuracy we won't need to compute any gradients; as a result we don't need PyTorch to build a computational graph for us when we compute scores. To prevent a graph from being built we scope our computation under a `torch.no_grad()` context manager, which will save some computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVcH3G0uaLhB"
   },
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model_fn, params):\n",
    "    \"\"\"\n",
    "    Check the accuracy of a classification model.\n",
    "\n",
    "    Inputs:\n",
    "    - loader: A DataLoader for the data split we want to check\n",
    "    - model_fn: A function that performs the forward pass of the model,\n",
    "      with the signature scores = model_fn(x, params)\n",
    "    - params: List of PyTorch Tensors giving parameters of the model\n",
    "\n",
    "    Returns: Nothing, but prints the accuracy of the model\n",
    "    \"\"\"\n",
    "    split = 'val' if loader.dataset.train else 'test'\n",
    "    print('Checking accuracy on the %s set' % split)\n",
    "    num_correct, num_samples = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.int64)\n",
    "            scores = model_fn(x, params)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhf6XwDeaLhB"
   },
   "source": [
    "## BareBones PyTorch: Training Loop\n",
    "We can now set up a basic training loop to train our network. We will train the model using stochastic gradient descent without momentum.\n",
    "\n",
    "The last piece of the puzzle we need is a loss function. Here, we will use `torch.functional.cross_entropy` to compute the loss; you can [read about it here](http://pytorch.org/docs/stable/nn.html#cross-entropy). Cross entropy is the go to loss function for multi-way classification, as opposed to MSE which tends to work better for regression tasks. Cross entropy in pytorch will take the softmax of our model's outputs for us and, as such, we don't need to include it explicitly in our models!\n",
    "\n",
    "The training loop takes as input the neural network function, a list of initialized parameters (`[w1, w2]` in our example), the learning rate and frequency of iterations to print information about the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8m9LGOk2aLhC"
   },
   "outputs": [],
   "source": [
    "def train(model_fn, params, learning_rate, print_every=100):\n",
    "    \"\"\"\n",
    "    Trains a model on CIFAR-10 for a single epoch.\n",
    "\n",
    "    Inputs:\n",
    "    - model_fn: A Python function that performs the forward pass of the model.\n",
    "      It should have the signature scores = model_fn(x, params) where x is a\n",
    "      PyTorch Tensor of image data, params is a list of PyTorch Tensors giving\n",
    "      model weights, and scores is a PyTorch Tensor of shape (N, C) giving\n",
    "      scores for the elements in x.\n",
    "    - params: List of PyTorch Tensors giving weights for the model\n",
    "    - learning_rate: Python scalar giving the learning rate to use for SGD\n",
    "    - print_every: Number of iterations at which the accuracy of the model\n",
    "      should be evaluated periodically\n",
    "\n",
    "    Returns: Nothing\n",
    "    \"\"\"\n",
    "    # Makes one pass through the training set\n",
    "    for t, (x, y) in enumerate(loader_train):\n",
    "        # Move the data to the proper device (GPU or CPU)\n",
    "        x = x.to(device=device, dtype=dtype)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "        # Forward pass: compute scores and loss\n",
    "        scores = model_fn(x, params)\n",
    "        loss = F.cross_entropy(scores, y)\n",
    "\n",
    "        # Backward pass: PyTorch figures out which Tensors in the computational\n",
    "        # graph has requires_grad=True and uses backpropagation to compute the\n",
    "        # gradient of the loss with respect to these Tensors, and stores the\n",
    "        # gradients in the .grad attribute of each Tensor.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update parameters. We don't want to backpropagate through the\n",
    "        # parameter updates, so we scope the updates under a torch.no_grad()\n",
    "        # context manager to prevent a computational graph from being built.\n",
    "        with torch.no_grad():\n",
    "            for w in params:\n",
    "                w -= learning_rate * w.grad\n",
    "\n",
    "                # Manually zero the gradients after running the backward pass\n",
    "                w.grad.zero_()\n",
    "\n",
    "        if t % print_every == 0:\n",
    "            print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "            check_accuracy(loader_val, model_fn, params)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oAKCo6praLhC"
   },
   "source": [
    "## BareBones PyTorch: Train a Two-Layer Network\n",
    "Now we are ready to run the training loop. We need to explicitly allocate tensors for the fully connected weights, `w1` and `w2`.\n",
    "\n",
    "Each minibatch of CIFAR has 64 examples, so the tensor shape is `[64, 3, 32, 32]`.\n",
    "\n",
    "After flattening, `x` shape should be `[64, 3 * 32 * 32]`. This will be the size of the first dimension of `w1`.\n",
    "The second dimension of `w1` is the hidden layer size, which will also be the first dimension of `w2`.\n",
    "\n",
    "Finally, the output of the network is a 10-dimensional vector that represents the probability distribution over 10 classes.\n",
    "\n",
    "You don't need to tune any hyperparameters but you should see accuracies above 40% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Pb0nXR5aLhC",
    "outputId": "65a37235-a8dd-4008-9b25-a010704e9c62"
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "w1 = random_weight((3 * 32 * 32, hidden_layer_size))\n",
    "w2 = random_weight((hidden_layer_size, 10))\n",
    "\n",
    "train(two_layer_fc, [w1, w2], learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM84kTmRaLhE"
   },
   "source": [
    "# 5. PyTorch nn.Module\n",
    "\n",
    "Similarly to our implementations in the previous part of the assignment, barebone PyTorch requires that we track all the parameter tensors by hand. This is fine for small networks with a few tensors, but it would be extremely inconvenient and error-prone to track tens or hundreds of tensors in larger networks.\n",
    "\n",
    "PyTorch provides the `nn.Module` API for you to define arbitrary network architectures, while tracking every learnable parameters for you. PyTorch also provides the `torch.optim` package that implements all the common optimizers, such as SGD, RMSProp, and Adam. It even supports approximate second-order methods like L-BFGS! You can refer to the [doc](http://pytorch.org/docs/master/optim.html) for the exact specifications of each optimizer.\n",
    "\n",
    "To use the Module API, we follow the steps below:\n",
    "\n",
    "1. Subclass `nn.Module`. Give your network class an intuitive name like `TwoLayerFC`.\n",
    "\n",
    "2. In the constructor `__init__()`, define all the layers you need as class attributes. Layer objects like `nn.Linear` and `nn.Conv2d` are themselves `nn.Module` subclasses and contain learnable parameters, so that you don't have to instantiate the raw tensors yourself. `nn.Module` will track these internal parameters for you. Refer to the [doc](http://pytorch.org/docs/master/nn.html) to learn more about the dozens of builtin layers. **Warning**: don't forget to call the `super().__init__()` first!\n",
    "\n",
    "3. In the `forward()` method, define the *connectivity* of your network. You should use the attributes defined in `__init__` as function calls that take tensor as input and output the \"transformed\" tensor. Do *not* create any new layers with learnable parameters in `forward()`! All of them must be declared upfront in `__init__`.\n",
    "\n",
    "After you define your Module subclass, you can instantiate it as an object and call it just like the NN forward function in part II."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBRcmHLBJyPg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eD2oTTAvuA2P"
   },
   "source": [
    "## Module API: Two-Layer Network\n",
    "Here is a concrete example of a 2-layer fully connected network, using the Kaiming normal initialization method for our weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hNdNZS9QaLhE",
    "outputId": "dffef815-6d21-4201-e0ab-c8938e8acd88"
   },
   "outputs": [],
   "source": [
    "class TwoLayerFC(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super().__init__()\n",
    "        # assign layer objects to class attributes\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # forward always defines connectivity\n",
    "        x = flatten(x)\n",
    "        scores = self.fc2(F.relu(self.fc1(x)))\n",
    "        return scores\n",
    "\n",
    "def test_TwoLayerFC():\n",
    "    input_size = 50\n",
    "    x = torch.zeros((64, input_size), dtype=dtype)  # minibatch size 64, feature dimension 50\n",
    "    model = TwoLayerFC(input_size, 42, 10)\n",
    "    scores = model(x)\n",
    "    print(scores.size())  # you should see [64, 10]\n",
    "test_TwoLayerFC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os_N5hL8aLhF"
   },
   "source": [
    "## Module API: Check Accuracy\n",
    "Given the validation or test set, we can check the classification accuracy of a neural network.\n",
    "\n",
    "This version is slightly different from the one above, since we do not manually pass in the parameters anymore now that modules can keep track of their own parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7o2xBUQSaLhF"
   },
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    if loader.dataset.train:\n",
    "        print('Checking accuracy on validation set')\n",
    "    else:\n",
    "        print('Checking accuracy on test set')\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "            scores = model(x)\n",
    "            _, preds = scores.max(1)\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_samples += preds.size(0)\n",
    "        acc = float(num_correct) / num_samples\n",
    "        print('Got %d / %d correct (%.2f)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QdtNnSgaLhF"
   },
   "source": [
    "## Module API: Training Loop\n",
    "We also use a slightly different training loop. Rather than updating the values of the weights ourselves, we use an `Optimizer` object from the `torch.optim` package [here](https://pytorch.org/docs/stable/optim.html), which abstract the notion of an optimization algorithm and provides implementations of most of the algorithms commonly used to optimize neural networks.\n",
    "\n",
    "In keeping with this more object oriented, higher level version of torch, we can also define a loss function that is an object that can be called to get your final loss tensor. Loss functions of this type can be found [here](https://pytorch.org/docs/stable/nn.html#loss-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0xcRng_7aLhF"
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, loader_train, loader_val, epochs=1, print_every=100):\n",
    "    \"\"\"\n",
    "    Train a model on CIFAR-10 using the PyTorch Module API and prints model\n",
    "    accuracies during training.\n",
    "\n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - loader_train: Dataloader for training\n",
    "    - loader_val: Dataloader for evaluation\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    - print_every: Number of iterations at which the accuracy of the model\n",
    "      should be evaluated periodically\n",
    "\n",
    "    Returns: Lists of validation accuracies at the end of each epoch.\n",
    "    \"\"\"\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    for e in range(epochs):\n",
    "        print('-' * 128)\n",
    "        for t, (x, y) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = loss_fn(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each trainable parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy(loader_val, model)\n",
    "                print()\n",
    "        val_accs.append(check_accuracy(loader_val, model))\n",
    "    return val_accs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7qwOTqAaLhF"
   },
   "source": [
    "## Module API: Train a Two-Layer Network\n",
    "Now we are ready to run the training loop. In contrast to the previous part, we don't explicitly allocate parameter tensors anymore.\n",
    "\n",
    "Simply pass the input size, hidden layer size, and number of classes (i.e. output size) to the constructor of `TwoLayerFC`.\n",
    "\n",
    "You also need to define an optimizer that tracks all the learnable parameters inside `TwoLayerFC`.\n",
    "\n",
    "You don't need to tune any hyperparameters, but you should see model accuracies above 40% after training for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DuUzgcYxaLhF",
    "outputId": "84f88042-59b6-4f3c-a56a-c8c0db5db4e1"
   },
   "outputs": [],
   "source": [
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "model = TwoLayerFC(3 * 32 * 32, hidden_layer_size, 10)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer, loader_train, loader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pffIrWumaLhG"
   },
   "source": [
    "# 6. PyTorch nn.Sequential\n",
    "\n",
    "The previous part introduced the PyTorch Module API, which allows you to define arbitrary learnable layers and their connectivity.\n",
    "\n",
    "For simple models like a stack of feed forward layers, you still need to go through 3 steps: subclass `nn.Module`, assign layers to class attributes in `__init__`, and call each layer one by one in `forward()`. While this API is very convenient for large and complex models, is there a more convenient way?\n",
    "\n",
    "Fortunately, PyTorch provides a container Module called `nn.Sequential`, which merges the above steps into one. It is not as flexible as `nn.Module`, because you cannot specify more complex topology than a feed-forward stack, but it's good enough for many use cases. This should feel familiar to the section on torch transforms since the `transforms.Compose` transform for containing multiple transforms at once sort of works in a similar way.\n",
    "\n",
    "### Sequential API: Two-Layer Network\n",
    "Let's see how to rewrite our two-layer fully connected network example with `nn.Sequential`, and train it using the training loop defined above.\n",
    "\n",
    "Again, you don't need to tune any hyperparameters here, but you shoud achieve above 40% accuracy after one epoch of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Auy0UEBuaLhG",
    "outputId": "26c1f885-9dcc-4e8f-d95b-56a83e95b4e5"
   },
   "outputs": [],
   "source": [
    "# We need to wrap `flatten` function in a module in order to stack it\n",
    "# in nn.Sequential\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return flatten(x)\n",
    "\n",
    "hidden_layer_size = 4000\n",
    "learning_rate = 1e-2\n",
    "\n",
    "model = nn.Sequential(\n",
    "    Flatten(),\n",
    "    nn.Linear(3 * 32 * 32, hidden_layer_size),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hidden_layer_size, 10),\n",
    ")\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "train(model, optimizer, loader_train, loader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTR-SVj5VcXW"
   },
   "source": [
    "# 7. Assignment:\n",
    "\n",
    "Now it's your job to train a model that achieves **at least 95%** accuracy on the MNIST **test** set within just 5 epochs.\n",
    "As a reminder, the [MNIST](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) dataset is the handwritten digits dataset that we saw in lecture: it consists of `28 x 28 x 1` images (i.e., each image is `28 x 28` grayscale) and there are 10 total classes (each digit from 0 to 9). The training set has 60,000 images and the testing set has 10,000 images. For this assignment, we will split the training set further into a slightly smaller training set of 50,000 images + a validation set of 10,000 images (to be used for hyperparameter tuning). We have taken care of the dataloading process for you, and you should only define your model and/or any hyperparameters you wish to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itA8Mu6YeJ4Z"
   },
   "source": [
    "Some of the hyperparameters you can play around with include the training data batch size, the choice of optimizer (and hence, the learning rate) and the network architecture itself. We have fixed the loss function to cross-entropy loss since we want to use the train and check_accuracy methods from before.\n",
    "\n",
    "Something to keep in mind: while you can change your batch size, if you make it too big, it might not fit into memory and you will run into OUT OF MEMORY errors. As such, your batch size is typically dictated by the amount of RAM you have available on the hardware used for training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRDn_2sieAiQ"
   },
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PZCyfHSwVgWl",
    "outputId": "1ffc8393-022d-499f-ecff-dea312678561"
   },
   "source": [
    "mnist_train = datasets.MNIST('.', download = True, train = True, transform = T.ToTensor())\n",
    "loader_train = DataLoader(mnist_train, batch_size=batch_size, sampler=sampler.SubsetRandomSampler(range(50000)))\n",
    "\n",
    "mnist_val = datasets.MNIST('.', download = True, train = True, transform = T.ToTensor())\n",
    "loader_val = DataLoader(mnist_val, batch_size=batch_size, sampler=sampler.SubsetRandomSampler(range(50000, 60000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DLWeAqOXgwhM",
    "outputId": "7127b90c-e804-47c5-8cff-0901b95276da"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loader_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51165/2516595640.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loader_train' is not defined"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader_train))\n",
    "print(batch[0].shape, batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX5cTn8-dLqs"
   },
   "source": [
    "Define your model here: you should use either the nn.Model or nn.Sequential API instead of the barebones framework. You need to make sure that your model can accept inputs of shape `(N, 1, 28, 28)` and outputs tensors of shape `(N, 10)` where `N` is an arbitrary batch size (you should not assume anything about it, even though you control it as a hyperparameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tQ2O8F1PdGjo"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_51165/3505929346.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = nn.Sequential(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    nn.Flatten(),  # Flatten the feature maps\n",
    "    nn.Linear(64 * 7 * 7, 128),  # Fully connected layer\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 10)  # Output layer with 10 classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8zw10mr4hf0e"
   },
   "source": [
    "You can use this piece of code to test if your input and output shapes work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "X9JbnZRlheUY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros((8, 1, 28, 28), dtype=dtype)  # minibatch size of 8\n",
    "scores = model(x)\n",
    "print(scores.size())  # you should see [8, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ6xxviJkyyJ"
   },
   "source": [
    "For fun, let's also check how big your network is! The following code block will return the number of trainable parameters (both weight matrices and bias vectors) in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "QvAl1NEGlItI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421642"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5Kus1_jDNhK"
   },
   "source": [
    "As a challenge, try to make your model as small as possible. For reference, the staff solution only used ~13k parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WlBvNe2d8Ed"
   },
   "source": [
    "Let's train our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "cu6oqvpqd4RX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "Iteration 0, loss = 2.3192\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 200, loss = 2.3086\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 400, loss = 2.3062\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 600, loss = 2.2884\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 800, loss = 2.3122\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1000, loss = 2.2950\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1200, loss = 2.3142\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1400, loss = 2.2961\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "Iteration 0, loss = 2.3002\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 200, loss = 2.3117\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 400, loss = 2.3036\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 600, loss = 2.3106\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 800, loss = 2.2921\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1000, loss = 2.3044\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1200, loss = 2.3006\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1400, loss = 2.2973\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "Iteration 0, loss = 2.3057\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 200, loss = 2.3358\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 400, loss = 2.2837\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 600, loss = 2.2858\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 800, loss = 2.2910\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1000, loss = 2.3006\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1200, loss = 2.2913\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1400, loss = 2.2939\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "Iteration 0, loss = 2.3178\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 200, loss = 2.3139\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 400, loss = 2.3137\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 600, loss = 2.3132\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 800, loss = 2.2918\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1000, loss = 2.2751\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1200, loss = 2.3125\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1400, loss = 2.3035\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "--------------------------------------------------------------------------------------------------------------------------------\n",
      "Iteration 0, loss = 2.2953\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 200, loss = 2.3061\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 400, loss = 2.3045\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 600, loss = 2.3102\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 800, loss = 2.3173\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1000, loss = 2.3078\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1200, loss = 2.3037\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Iteration 1400, loss = 2.3025\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n",
      "\n",
      "Checking accuracy on validation set\n",
      "Got 988 / 10000 correct (9.88)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0988, 0.0988, 0.0988, 0.0988, 0.0988]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "train(model, optimizer, loader_train, loader_val, epochs=5, print_every=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G1CuT568TWLt"
   },
   "source": [
    "## Autograder and Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WUJnUAyoePa"
   },
   "source": [
    "After you feel confident that you have a decent model, run the cell below.\n",
    "\n",
    "Feel free to read the code block but **PLEASE DO NOT TOUCH IT**: this will produce a pickle file that will contain your model's predictions on the MNIST test set --- tampering with the code block below might mess up the file that you will submit to the Gradescope autograder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Vzfm7AtGTU0q"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "mnist_test = datasets.MNIST('.', download = True, train = False, transform = T.ToTensor())\n",
    "loader_test = DataLoader(mnist_test, batch_size=32, shuffle=False)\n",
    "\n",
    "model.eval()  # set model to evaluation mode\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for x, _ in loader_test:\n",
    "        x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "        scores = model(x)\n",
    "        _, preds = scores.max(1)\n",
    "        predictions.append(preds)\n",
    "predictions = torch.cat(predictions).tolist()\n",
    "with open(\"my_predictions.pickle\", \"wb\") as file:\n",
    "    pickle.dump(predictions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "WuQhjQNjaLg0",
    "RMhHaMi0aLg2",
    "qE3x7fHVaLg6",
    "V97omoMxaLg5",
    "Aw9DIYO3pY-6",
    "CWkPUL8LpUS6",
    "NBkxHGmrpM23",
    "t6lLn85vpIvV",
    "PqIGRx_VaLg9",
    "2qZThopiaLg-",
    "1iZyMy3uaLhA",
    "ZDCsM6PfaLhB",
    "fhf6XwDeaLhB",
    "oAKCo6praLhC",
    "LM84kTmRaLhE"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
